{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "annotation_list = [\"Tweet Annotations - Feb Translated Annotations.tsv\"]\n",
    "annotation_list.append(\"Tweet Annotations - Mar Translated Annotations.tsv\")\n",
    "annotation_list.append(\"Tweet Annotations - Apr Translated Annotations.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer the contents of a tsv file to a list for easier access\n",
    "def tsv_to_list(annotation_file):\n",
    "    annotation_list = []\n",
    "    annotation = open(annotation_file, encoding='utf-8')\n",
    "    read_tsv = csv.reader(annotation, delimiter=\"\\t\") \n",
    "    for row in read_tsv:\n",
    "        annotation_list.append(row)\n",
    "    return annotation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the label for each verse for one annotator\n",
    "# N: -1, N+NU: -0.5, NU: 0, P+NU: 0.5, P: 1\n",
    "def get_labels_for(annotation):\n",
    "    tweets_and_labels = []\n",
    "    row_num = 0\n",
    "    for row in annotation:\n",
    "        if row_num == 0:\n",
    "            row_num += 1\n",
    "            continue\n",
    "        if row[15] != '': # don't use unsure\n",
    "            row_num += 1\n",
    "            continue\n",
    "            #print(\"not using unsure row\", row_num+1)\n",
    "        if 'UNUSABLE' in row[16].upper() or 'NOT ENOUGH INFO' in row[16].upper():\n",
    "            row_num += 1\n",
    "            continue\n",
    "            #print(\"unusable row\", row_num+1)\n",
    "        elif row[6].upper() == 'X' and (row[8].upper() != 'X' and row[7].upper() == 'X'): # don't use pos + neg mixed labels\n",
    "            row_num += 1\n",
    "            continue\n",
    "            #print(\"not using pos+neg row\", row_num+1)\n",
    "        elif row[6].upper() == 'X' and (row[7].upper() != 'X' and row[8].upper() != 'X'): # only pos\n",
    "            new_row = [annotation[row_num][0], annotation[row_num][1], annotation[row_num][2], annotation[row_num][3], annotation[row_num][4], annotation[row_num][5], 1]\n",
    "            tweets_and_labels.append(new_row)\n",
    "        #elif row[6].upper() == 'X' and (row[8].upper() == 'X' and row[7].upper() != 'X'): # pos + neu\n",
    "        #    new_row = [annotation[row_num][0], annotation[row_num][1], annotation[row_num][2], annotation[row_num][3], annotation[row_num][4], annotation[row_num][5], 3]\n",
    "        #    tweets_and_labels.append(new_row)\n",
    "        elif row[8].upper() == 'X' and (row[6].upper() != 'X' and row[7].upper() != 'X'): # only neu \n",
    "            new_row = [annotation[row_num][0], annotation[row_num][1], annotation[row_num][2], annotation[row_num][3], annotation[row_num][4], annotation[row_num][5], 0]\n",
    "            tweets_and_labels.append(new_row)\n",
    "        elif row[7].upper() == 'X' and (row[8].upper() != 'X' and row[6].upper() != 'X'): # only neg\n",
    "            new_row = [annotation[row_num][0], annotation[row_num][1], annotation[row_num][2], annotation[row_num][3], annotation[row_num][4], annotation[row_num][5], -1]\n",
    "            tweets_and_labels.append(new_row)\n",
    "        #elif row[8].upper() == 'X' and (row[7].upper() == 'X' and row[6].upper() != 'X'): # neg + neu\n",
    "        #    new_row = [annotation[row_num][0], annotation[row_num][1], annotation[row_num][2], annotation[row_num][3], annotation[row_num][4], annotation[row_num][5], 2]\n",
    "        #    tweets_and_labels.append(new_row) \n",
    "        #else:\n",
    "        #    print(\"outlier row:\", row_num+1)\n",
    "        row_num += 1\n",
    "    return tweets_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feb_list = tsv_to_list(annotation_list[0])\n",
    "feb_labels = get_labels_for(feb_list)\n",
    "mar_list = tsv_to_list(annotation_list[1])\n",
    "mar_labels = get_labels_for(mar_list)\n",
    "apr_list = tsv_to_list(annotation_list[2])\n",
    "apr_labels = get_labels_for(apr_list)\n",
    "\n",
    "all_labels = feb_labels + mar_labels + apr_labels\n",
    "#print(all_labels.count(-1), all_labels.count(0), all_labels.count(1))\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.DataFrame(all_labels, columns=['tweetID', 'tweetTime', 'tweetText', 'tweetLang', 'tweetCoordinates', 'tweetPlace', 'tweetSentiment'])\n",
    "data.head()\n",
    "\n",
    "test_data = pd.DataFrame(apr_labels, columns=['tweetID', 'tweetTime', 'tweetText', 'tweetLang', 'tweetCoordinates', 'tweetPlace', 'tweetSentiment'])\n",
    "\n",
    "# Drop 'airline' column\n",
    "#data.drop(['airline'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4177\n",
      "4177\n"
     ]
    }
   ],
   "source": [
    "# Get all the words from the data\n",
    "import random\n",
    "\n",
    "text = []\n",
    "for tweet in data['tweetText'].values:\n",
    "    text.append(tweet.split())\n",
    "print(len(text))\n",
    "\n",
    "tweets_labels = []\n",
    "tweet_count = 0\n",
    "for label in data['tweetSentiment']:\n",
    "    if tweet_count < len(data['tweetSentiment']):\n",
    "        tweets_labels.append((text[tweet_count], label))\n",
    "        tweet_count += 1\n",
    "print(len(tweets_labels))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(tweets_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'to', 'look', 'at', \"people's\", 'opinions', 'about', 'the', 'coronavirus', 'because', 'can', 'find', 'both', 'memes', 'and', 'people', 'who', 'believe', 'that']\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "\n",
    "for sent in text:\n",
    "    for w in sent:\n",
    "        all_words.append(w.lower())\n",
    "\n",
    "all_words = FreqDist(all_words)\n",
    "\n",
    "word_features = list(all_words.keys())[:3000]\n",
    "print(word_features[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "        #print(w, \":\", features[w])\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(find_features(tweet), category) for (tweet, category) in tweets_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3341\n"
     ]
    }
   ],
   "source": [
    "cutoff = round(len(tweets_labels)*0.8) - 1\n",
    "training_set = featuresets[:cutoff]\n",
    "testing_set = featuresets[cutoff:]\n",
    "print(cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 57.89473684210527\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier accuracy percent:\",(accuracy(classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = 'Just 9 cases, 7 of them linked, from 60,000 tests! Outstanding! NSW absolutely smashing it  Thanks so much to our brilliant contact tracers'\n",
    "classifier.classify(find_features(tweet.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
