{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "annotation_list = [\"Tweet Annotations - Feb Translated Annotations.tsv\"]\n",
    "annotation_list.append(\"Tweet Annotations - Mar Translated Annotations.tsv\")\n",
    "annotation_list.append(\"Tweet Annotations - Apr Translated Annotations.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer the contents of a tsv file to a list for easier access\n",
    "def tsv_to_list(annotation_file):\n",
    "    annotation_list = []\n",
    "    annotation = open(annotation_file, encoding='utf-8')\n",
    "    read_tsv = csv.reader(annotation, delimiter=\"\\t\") \n",
    "    for row in read_tsv:\n",
    "        annotation_list.append(row)\n",
    "    return annotation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the label for each verse for one annotator\n",
    "# N: -1, N+NU: -0.5, NU: 0, P+NU: 0.5, P: 1\n",
    "def get_labels_for(annotation):\n",
    "    tweets_and_labels = []\n",
    "    row_num = 0\n",
    "    for row in annotation:\n",
    "        if row_num == 0:\n",
    "            row_num += 1\n",
    "            continue\n",
    "        if row[15] != '': # don't use unsure\n",
    "            row_num += 1\n",
    "            continue\n",
    "            #print(\"not using unsure row\", row_num+1)\n",
    "        if 'UNUSABLE' in row[16].upper() or 'NOT ENOUGH INFO' in row[16].upper():\n",
    "            row_num += 1\n",
    "            continue\n",
    "            #print(\"unusable row\", row_num+1)\n",
    "        elif row[6].upper() == 'X' and (row[8].upper() != 'X' and row[7].upper() == 'X'): # don't use pos + neg mixed labels\n",
    "            row_num += 1\n",
    "            continue\n",
    "            #print(\"not using pos+neg row\", row_num+1)\n",
    "        elif row[6].upper() == 'X' and (row[7].upper() != 'X' and row[8].upper() != 'X'): # only pos\n",
    "            new_row = [annotation[row_num][0], annotation[row_num][1], annotation[row_num][2], annotation[row_num][3], annotation[row_num][4], annotation[row_num][5], 1]\n",
    "            tweets_and_labels.append(new_row)\n",
    "        #elif row[6].upper() == 'X' and (row[8].upper() == 'X' and row[7].upper() != 'X'): # pos + neu\n",
    "        #    new_row = [annotation[row_num][0], annotation[row_num][1], annotation[row_num][2], annotation[row_num][3], annotation[row_num][4], annotation[row_num][5], 3]\n",
    "        #    tweets_and_labels.append(new_row)\n",
    "        elif row[8].upper() == 'X' and (row[6].upper() != 'X' and row[7].upper() != 'X'): # only neu \n",
    "            new_row = [annotation[row_num][0], annotation[row_num][1], annotation[row_num][2], annotation[row_num][3], annotation[row_num][4], annotation[row_num][5], 0]\n",
    "            tweets_and_labels.append(new_row)\n",
    "        elif row[7].upper() == 'X' and (row[8].upper() != 'X' and row[6].upper() != 'X'): # only neg\n",
    "            new_row = [annotation[row_num][0], annotation[row_num][1], annotation[row_num][2], annotation[row_num][3], annotation[row_num][4], annotation[row_num][5], -1]\n",
    "            tweets_and_labels.append(new_row)\n",
    "        #elif row[8].upper() == 'X' and (row[7].upper() == 'X' and row[6].upper() != 'X'): # neg + neu\n",
    "        #    new_row = [annotation[row_num][0], annotation[row_num][1], annotation[row_num][2], annotation[row_num][3], annotation[row_num][4], annotation[row_num][5], 2]\n",
    "        #    tweets_and_labels.append(new_row) \n",
    "        #else:\n",
    "        #    print(\"outlier row:\", row_num+1)\n",
    "        row_num += 1\n",
    "    return tweets_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feb_list = tsv_to_list(annotation_list[0])\n",
    "feb_labels = get_labels_for(feb_list)\n",
    "mar_list = tsv_to_list(annotation_list[1])\n",
    "mar_labels = get_labels_for(mar_list)\n",
    "apr_list = tsv_to_list(annotation_list[2])\n",
    "apr_labels = get_labels_for(apr_list)\n",
    "\n",
    "all_labels = feb_labels + mar_labels + apr_labels\n",
    "#print(all_labels.count(-1), all_labels.count(0), all_labels.count(1))\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.DataFrame(all_labels, columns=['tweetID', 'tweetTime', 'tweetText', 'tweetLang', 'tweetCoordinates', 'tweetPlace', 'tweetSentiment'])\n",
    "data.head()\n",
    "\n",
    "test_data = pd.DataFrame(apr_labels, columns=['tweetID', 'tweetTime', 'tweetText', 'tweetLang', 'tweetCoordinates', 'tweetPlace', 'tweetSentiment'])\n",
    "\n",
    "# Drop 'airline' column\n",
    "#data.drop(['airline'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['I', 'love', 'to', 'look', 'at', \"people's\", 'opinions', 'about', 'the', 'coronavirus', 'because', 'I', 'can', 'find', 'both', 'memes', 'and', 'people', 'who', 'believe', 'that', 'this', 'will', 'be', 'World', 'War', 'Z', 'I', 'also', 'love', 'Spain', 'for', 'doing', 'these', 'trends', 'tt'], 1)\n"
     ]
    }
   ],
   "source": [
    "# Get all the words from the data\n",
    "import random\n",
    "\n",
    "text = []\n",
    "for tweet in data['tweetText'].values:\n",
    "    text.append(tweet.split())\n",
    "\n",
    "documents = [(words, label) for words in text for label in data['tweetSentiment']]\n",
    "print(documents[0])\n",
    "\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'to', 'look', 'at', \"people's\", 'opinions', 'about', 'the', 'coronavirus', 'because', 'can', 'find', 'both', 'memes', 'and', 'people', 'who', 'believe', 'that']\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "\n",
    "for sent in text:\n",
    "    for w in sent:\n",
    "        all_words.append(w.lower())\n",
    "\n",
    "all_words = FreqDist(all_words)\n",
    "\n",
    "word_features = list(all_words.keys())[:3000]\n",
    "print(word_features[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-4ebb4e2111e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeaturesets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfind_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-4ebb4e2111e1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeaturesets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfind_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-6c793c1bda73>\u001b[0m in \u001b[0;36mfind_features\u001b[1;34m(document)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
