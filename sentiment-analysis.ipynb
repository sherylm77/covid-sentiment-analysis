{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "annotation_list = [\"Tweet Annotations - Feb Translated Annotations.tsv\"]\n",
    "annotation_list.append(\"Tweet Annotations - Mar Translated Annotations.tsv\")\n",
    "annotation_list.append(\"Tweet Annotations - Apr Translated Annotations.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer the contents of a tsv file to a list for easier access\n",
    "def tsv_to_list(annotation_file):\n",
    "    annotation_list = []\n",
    "    annotation = open(annotation_file, encoding='utf-8')\n",
    "    read_tsv = csv.reader(annotation, delimiter=\"\\t\") \n",
    "    for row in read_tsv:\n",
    "        annotation_list.append(row)\n",
    "    return annotation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the label for each verse for one annotator\n",
    "# N: -1, N+NU: -0.5, NU: 0, P+NU: 0.5, P: 1\n",
    "def get_labels_for(annotation):\n",
    "    tweets_and_labels = []\n",
    "    row_num = 0\n",
    "    for row in annotation:\n",
    "        if row_num == 0:\n",
    "            row_num += 1\n",
    "            continue\n",
    "        if row[15] != '': # don't use unsure\n",
    "            row_num += 1\n",
    "            continue\n",
    "            #print(\"not using unsure row\", row_num+1)\n",
    "        if 'UNUSABLE' in row[16].upper() or 'NOT ENOUGH INFO' in row[16].upper():\n",
    "            row_num += 1\n",
    "            continue\n",
    "            #print(\"unusable row\", row_num+1)\n",
    "        elif row[6].upper() == 'X' and (row[8].upper() != 'X' and row[7].upper() == 'X'): # don't use pos + neg mixed labels\n",
    "            row_num += 1\n",
    "            continue\n",
    "            #print(\"not using pos+neg row\", row_num+1)\n",
    "        elif row[6].upper() == 'X' and (row[7].upper() != 'X' and row[8].upper() != 'X'): # only pos\n",
    "            new_row = [annotation[row_num][0], annotation[row_num][1], annotation[row_num][2], annotation[row_num][3], annotation[row_num][4], annotation[row_num][5], 2]\n",
    "            tweets_and_labels.append(new_row)\n",
    "        #elif row[6].upper() == 'X' and (row[8].upper() == 'X' and row[7].upper() != 'X'): # pos + neu\n",
    "        #    new_row = [annotation[row_num][0], annotation[row_num][1], annotation[row_num][2], annotation[row_num][3], annotation[row_num][4], annotation[row_num][5], 3]\n",
    "        #    tweets_and_labels.append(new_row)\n",
    "        elif row[8].upper() == 'X' and (row[6].upper() != 'X' and row[7].upper() != 'X'): # only neu \n",
    "            new_row = [annotation[row_num][0], annotation[row_num][1], annotation[row_num][2], annotation[row_num][3], annotation[row_num][4], annotation[row_num][5], 1]\n",
    "            tweets_and_labels.append(new_row)\n",
    "        elif row[7].upper() == 'X' and (row[8].upper() != 'X' and row[6].upper() != 'X'): # only neg\n",
    "            new_row = [annotation[row_num][0], annotation[row_num][1], annotation[row_num][2], annotation[row_num][3], annotation[row_num][4], annotation[row_num][5], 0]\n",
    "            tweets_and_labels.append(new_row)\n",
    "        #elif row[8].upper() == 'X' and (row[7].upper() == 'X' and row[6].upper() != 'X'): # neg + neu\n",
    "        #    new_row = [annotation[row_num][0], annotation[row_num][1], annotation[row_num][2], annotation[row_num][3], annotation[row_num][4], annotation[row_num][5], 2]\n",
    "        #    tweets_and_labels.append(new_row) \n",
    "        #else:\n",
    "        #    print(\"outlier row:\", row_num+1)\n",
    "        row_num += 1\n",
    "    return tweets_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetID</th>\n",
       "      <th>tweetTime</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>tweetLang</th>\n",
       "      <th>tweetCoordinates</th>\n",
       "      <th>tweetPlace</th>\n",
       "      <th>tweetSentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1232073974093352966</td>\n",
       "      <td>2020-02-24 22:45:00</td>\n",
       "      <td>I love to look at people's opinions about the ...</td>\n",
       "      <td>es</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1223738854596927488</td>\n",
       "      <td>2020-02-01 22:44:13</td>\n",
       "      <td>The latest The AmericanAwaken ing Daily PM!  T...</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1224564516748283904</td>\n",
       "      <td>2020-02-04 5:25:06</td>\n",
       "      <td>#Macau casinos will be closed for half a month...</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1232651956528799747</td>\n",
       "      <td>2020-02-26 13:01:42</td>\n",
       "      <td>Social media conspiracies blame coronavirus on...</td>\n",
       "      <td>ca</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1231469470020972545</td>\n",
       "      <td>2020-02-23 6:42:55</td>\n",
       "      <td>Coronavirus, official: patient zero has nothin...</td>\n",
       "      <td>it</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tweetID            tweetTime  \\\n",
       "0  1232073974093352966  2020-02-24 22:45:00   \n",
       "1  1223738854596927488  2020-02-01 22:44:13   \n",
       "2  1224564516748283904   2020-02-04 5:25:06   \n",
       "3  1232651956528799747  2020-02-26 13:01:42   \n",
       "4  1231469470020972545   2020-02-23 6:42:55   \n",
       "\n",
       "                                           tweetText tweetLang  \\\n",
       "0  I love to look at people's opinions about the ...        es   \n",
       "1  The latest The AmericanAwaken ing Daily PM!  T...        en   \n",
       "2  #Macau casinos will be closed for half a month...        en   \n",
       "3  Social media conspiracies blame coronavirus on...        ca   \n",
       "4  Coronavirus, official: patient zero has nothin...        it   \n",
       "\n",
       "  tweetCoordinates tweetPlace  tweetSentiment  \n",
       "0                                           2  \n",
       "1                                           0  \n",
       "2                                           0  \n",
       "3                                           0  \n",
       "4                                           1  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feb_list = tsv_to_list(annotation_list[0])\n",
    "feb_labels = get_labels_for(feb_list)\n",
    "mar_list = tsv_to_list(annotation_list[1])\n",
    "mar_labels = get_labels_for(mar_list)\n",
    "apr_list = tsv_to_list(annotation_list[2])\n",
    "apr_labels = get_labels_for(apr_list)\n",
    "\n",
    "all_labels = feb_labels + mar_labels + apr_labels\n",
    "#print(all_labels.count(-1), all_labels.count(0), all_labels.count(1))\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(all_labels, columns=['tweetID', 'tweetTime', 'tweetText', 'tweetLang', 'tweetCoordinates', 'tweetPlace', 'tweetSentiment'])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow) (1.30.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: astunparse==1.6.3 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow) (2.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow) (3.12.2)\n",
      "Requirement already satisfied: scipy==1.4.1 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow) (1.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow) (0.9.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow) (1.19.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from protobuf>=3.8.0->tensorflow) (47.1.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.27.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
      "Name: tensorflow\n",
      "Version: 2.2.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\n",
      "Requires: grpcio, tensorboard, termcolor, keras-preprocessing, opt-einsum, google-pasta, numpy, wrapt, six, scipy, absl-py, h5py, protobuf, wheel, tensorflow-estimator, astunparse, gast\n",
      "Required-by: \n",
      "Requirement already satisfied: keras in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.4.3)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from keras) (1.19.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: six in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from h5py->keras) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "!pip3 install tensorflow\n",
    "!pip3 show tensorflow\n",
    "!pip3 install keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "Collecting click\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (0.16.0)\n",
      "Collecting regex\n",
      "  Downloading regex-2020.11.13-cp38-cp38-win_amd64.whl (270 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.58.0-py2.py3-none-any.whl (73 kB)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py): started\n",
      "  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434677 sha256=a98f54150389291c50df095889b7dfa8773f1f771b83be3763c891d08da36a76\n",
      "  Stored in directory: c:\\users\\steffi\\appdata\\local\\pip\\cache\\wheels\\ff\\d5\\7b\\f1fb4e1e1603b2f01c2424dd60fbcc50c12ef918bafc44b155\n",
      "Successfully built nltk\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-7.1.2 nltk-3.5 regex-2020.11.13 tqdm-4.58.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Steffi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk\n",
    "import nltk\n",
    "# Uncomment to download \"stopwords\"\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def text_preprocessing(s):\n",
    "    \"\"\"\n",
    "    - Lowercase the sentence\n",
    "    - Change \"'t\" to \"not\"\n",
    "    - Remove \"@name\"\n",
    "    - Isolate and remove punctuations except \"?\"\n",
    "    - Remove other special characters\n",
    "    - Remove stop words except \"not\" and \"can\"\n",
    "    - Remove trailing whitespace\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    # Change 't to 'not'\n",
    "    s = re.sub(r\"\\'t\", \" not\", s)\n",
    "    # Remove @name\n",
    "    s = re.sub(r'(@.*?)[\\s]', ' ', s)\n",
    "    # Isolate and remove punctuations except '?'\n",
    "    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\/\\,])', r' \\1 ', s)\n",
    "    s = re.sub(r'[^\\w\\s\\?]', ' ', s)\n",
    "    # Remove some special characters\n",
    "    s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n",
    "    # Remove stopwords except 'not' and 'can'\n",
    "    s = \" \".join([word for word in s.split()\n",
    "                  if word not in stopwords.words('english')\n",
    "                  or word in ['not', 'can']])\n",
    "    # Remove trailing whitespace\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "X = df['tweetText'].values\n",
    "X= X.astype(str)\n",
    "y = df['tweetSentiment'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "X_train = np.array([text_preprocessing(text) for text in X_train])\n",
    "X_test = np.array([text_preprocessing(text) for text in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer() \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "num_words=9000\n",
    "tokenizer.word_index = {e:i for e,i in tokenizer.word_index.items() if i <= num_words} # <= because tokenizer is 1 indexed\n",
    "tokenizer.word_index[tokenizer.oov_token] = num_words + 1\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "maxlen = 278\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding accuracy: 0.8438124861141968\n"
     ]
    }
   ],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath,encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word]\n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix\n",
    "    \n",
    "embedding_dim = 300\n",
    "embedding_matrix = create_embedding_matrix('glove.6B.300d.txt', tokenizer.word_index, embedding_dim)\n",
    "\n",
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "embedding_accuracy = nonzero_elements / vocab_size\n",
    "print('embedding accuracy: ' + str(embedding_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 278, 300)          2700600   \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 276, 256)          230656    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 276, 256)          0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 2,931,513\n",
      "Trainable params: 2,931,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=maxlen, trainable=True))\n",
    "model.add(layers.Conv1D(256, 3, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "opt = optimizers.Adam(lr=0.0002)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "34/34 [==============================] - 12s 347ms/step - loss: 0.6384 - val_loss: 0.5581\n",
      "Epoch 2/15\n",
      "34/34 [==============================] - 13s 374ms/step - loss: 0.4303 - val_loss: 0.4712\n",
      "Epoch 3/15\n",
      "34/34 [==============================] - 14s 402ms/step - loss: 0.2886 - val_loss: 0.3989\n",
      "Epoch 4/15\n",
      "34/34 [==============================] - 13s 397ms/step - loss: 0.1673 - val_loss: 0.3384\n",
      "Epoch 5/15\n",
      "34/34 [==============================] - 14s 414ms/step - loss: 0.0430 - val_loss: 0.2798\n",
      "Epoch 6/15\n",
      "34/34 [==============================] - 15s 431ms/step - loss: -0.0791 - val_loss: 0.2331\n",
      "Epoch 7/15\n",
      "34/34 [==============================] - 15s 447ms/step - loss: -0.2167 - val_loss: 0.1780\n",
      "Epoch 8/15\n",
      "34/34 [==============================] - 14s 412ms/step - loss: -0.3633 - val_loss: 0.1300\n",
      "Epoch 9/15\n",
      "34/34 [==============================] - 14s 399ms/step - loss: -0.5361 - val_loss: 0.0744\n",
      "Epoch 10/15\n",
      "34/34 [==============================] - 14s 400ms/step - loss: -0.7191 - val_loss: 0.0113\n",
      "Epoch 11/15\n",
      "34/34 [==============================] - 14s 398ms/step - loss: -0.9435 - val_loss: -0.0451\n",
      "Epoch 12/15\n",
      "34/34 [==============================] - 14s 399ms/step - loss: -1.1991 - val_loss: -0.1108\n",
      "Epoch 13/15\n",
      "34/34 [==============================] - 14s 397ms/step - loss: -1.5131 - val_loss: -0.1804\n",
      "Epoch 14/15\n",
      "34/34 [==============================] - 14s 398ms/step - loss: -1.8835 - val_loss: -0.2638\n",
      "Epoch 15/15\n",
      "34/34 [==============================] - 14s 407ms/step - loss: -2.3168 - val_loss: -0.3567\n"
     ]
    }
   ],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "         ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "res = model.fit(X_train, y_train, epochs=15, verbose=True, callbacks=callbacks, validation_data=(X_test, y_test), batch_size=100)\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.1000, Precision: 0.3684, Recall: 0.4186, F1-measure: 0.3115\n",
      "Threshold: 0.2000, Precision: 0.3650, Recall: 0.4249, F1-measure: 0.3253\n",
      "Threshold: 0.2500, Precision: 0.3620, Recall: 0.4234, F1-measure: 0.3271\n",
      "Threshold: 0.3000, Precision: 0.3638, Recall: 0.4290, F1-measure: 0.3347\n",
      "Threshold: 0.4000, Precision: 0.3657, Recall: 0.4367, F1-measure: 0.3457\n",
      "Threshold: 0.5000, Precision: 0.3654, Recall: 0.4425, F1-measure: 0.3540\n",
      "Threshold: 0.6000, Precision: 0.3641, Recall: 0.4437, F1-measure: 0.3582\n",
      "Threshold: 0.7000, Precision: 0.3642, Recall: 0.4462, F1-measure: 0.3637\n",
      "Threshold: 0.8000, Precision: 0.3577, Recall: 0.4377, F1-measure: 0.3626\n",
      "Threshold: 0.9000, Precision: 0.3506, Recall: 0.4325, F1-measure: 0.3692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\steffi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "thresholds=[0.1,0.2,0.25,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for val in thresholds:\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='macro')\n",
    "    recall = recall_score(y_test, pred, average='macro')\n",
    "    f1 = f1_score(y_test, pred, average='macro')\n",
    "   \n",
    "    print(\"Threshold: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(val, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion: neg, Precision: 0.5444, Recall: 0.4450, F1-measure: 0.4319\n",
      "Emotion: neu, Precision: 0.5444, Recall: 0.4450, F1-measure: 0.4319\n",
      "Emotion: pos, Precision: 0.5444, Recall: 0.4450, F1-measure: 0.4319\n"
     ]
    }
   ],
   "source": [
    "column_names = ['neg', 'neu', 'pos']\n",
    "f1_scores = []\n",
    "threshold = 0.7\n",
    "for i in range(0,3):\n",
    "    emotion_prediction = y_pred #[:,i]\n",
    "    emotion_prediction[emotion_prediction>=threshold]=1\n",
    "    emotion_prediction[emotion_prediction<threshold]=0\n",
    "    emotion_test = y_test #[:,i]\n",
    "    precision = precision_score(emotion_test, emotion_prediction, average='weighted')\n",
    "    recall = recall_score(emotion_test, emotion_prediction, average='weighted')\n",
    "    f1 = f1_score(emotion_test, emotion_prediction, average='weighted')\n",
    "    f1_scores.append(f1)\n",
    "    print(\"Emotion: {}, Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(column_names[i], precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "plt.bar(column_names,f1_scores)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
