{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "annotation_list = [\"Tweet Annotations - Feb Translated Annotations.tsv\"]\n",
    "annotation_list.append(\"Tweet Annotations - Mar Translated Annotations.tsv\")\n",
    "annotation_list.append(\"Tweet Annotations - Apr Translated Annotations.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer the contents of a tsv file to a list for easier access\n",
    "def tsv_to_list(annotation_file):\n",
    "    annotation_list = []\n",
    "    annotation = open(annotation_file, encoding='utf-8')\n",
    "    read_tsv = csv.reader(annotation, delimiter=\"\\t\") \n",
    "    for row in read_tsv:\n",
    "        annotation_list.append(row)\n",
    "    return annotation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the label for each verse for one annotator\n",
    "# N: -1, N+NU: -0.5, NU: 0, P+NU: 0.5, P: 1\n",
    "def get_labels_for(annotation):\n",
    "    tweets_and_labels = []\n",
    "    row_num = 0\n",
    "    for row in annotation:\n",
    "        if row_num == 0:\n",
    "            row_num += 1\n",
    "            continue\n",
    "        if row[15] != '': # don't use unsure\n",
    "            row_num += 1\n",
    "            continue\n",
    "            #print(\"not using unsure row\", row_num+1)\n",
    "        if 'UNUSABLE' in row[16].upper() or 'NOT ENOUGH INFO' in row[16].upper():\n",
    "            row_num += 1\n",
    "            continue\n",
    "            #print(\"unusable row\", row_num+1)\n",
    "        elif row[6].upper() == 'X' and (row[8].upper() != 'X' and row[7].upper() == 'X'): # don't use pos + neg mixed labels\n",
    "            row_num += 1\n",
    "            continue\n",
    "            #print(\"not using pos+neg row\", row_num+1)\n",
    "        elif row[6].upper() == 'X' and (row[7].upper() != 'X' and row[8].upper() != 'X'): # only pos\n",
    "            new_row = [annotation[row_num][0], annotation[row_num][1], annotation[row_num][2], annotation[row_num][3], annotation[row_num][4], annotation[row_num][5], 1]\n",
    "            tweets_and_labels.append(new_row)\n",
    "        #elif row[6].upper() == 'X' and (row[8].upper() == 'X' and row[7].upper() != 'X'): # pos + neu\n",
    "        #    new_row = [annotation[row_num][0], annotation[row_num][1], annotation[row_num][2], annotation[row_num][3], annotation[row_num][4], annotation[row_num][5], 3]\n",
    "        #    tweets_and_labels.append(new_row)\n",
    "        elif row[8].upper() == 'X' and (row[6].upper() != 'X' and row[7].upper() != 'X'): # only neu \n",
    "            new_row = [annotation[row_num][0], annotation[row_num][1], annotation[row_num][2], annotation[row_num][3], annotation[row_num][4], annotation[row_num][5], 0]\n",
    "            tweets_and_labels.append(new_row)\n",
    "        elif row[7].upper() == 'X' and (row[8].upper() != 'X' and row[6].upper() != 'X'): # only neg\n",
    "            new_row = [annotation[row_num][0], annotation[row_num][1], annotation[row_num][2], annotation[row_num][3], annotation[row_num][4], annotation[row_num][5], -1]\n",
    "            tweets_and_labels.append(new_row)\n",
    "        #elif row[8].upper() == 'X' and (row[7].upper() == 'X' and row[6].upper() != 'X'): # neg + neu\n",
    "        #    new_row = [annotation[row_num][0], annotation[row_num][1], annotation[row_num][2], annotation[row_num][3], annotation[row_num][4], annotation[row_num][5], 2]\n",
    "        #    tweets_and_labels.append(new_row) \n",
    "        #else:\n",
    "        #    print(\"outlier row:\", row_num+1)\n",
    "        row_num += 1\n",
    "    return tweets_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetID</th>\n",
       "      <th>tweetTime</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>tweetLang</th>\n",
       "      <th>tweetCoordinates</th>\n",
       "      <th>tweetPlace</th>\n",
       "      <th>tweetSentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1232073974093352966</td>\n",
       "      <td>2020-02-24 22:45:00</td>\n",
       "      <td>I love to look at people's opinions about the ...</td>\n",
       "      <td>es</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1223738854596927488</td>\n",
       "      <td>2020-02-01 22:44:13</td>\n",
       "      <td>The latest The AmericanAwaken ing Daily PM!  T...</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1224564516748283904</td>\n",
       "      <td>2020-02-04 5:25:06</td>\n",
       "      <td>#Macau casinos will be closed for half a month...</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1232651956528799747</td>\n",
       "      <td>2020-02-26 13:01:42</td>\n",
       "      <td>Social media conspiracies blame coronavirus on...</td>\n",
       "      <td>ca</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1231469470020972545</td>\n",
       "      <td>2020-02-23 6:42:55</td>\n",
       "      <td>Coronavirus, official: patient zero has nothin...</td>\n",
       "      <td>it</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tweetID            tweetTime  \\\n",
       "0  1232073974093352966  2020-02-24 22:45:00   \n",
       "1  1223738854596927488  2020-02-01 22:44:13   \n",
       "2  1224564516748283904   2020-02-04 5:25:06   \n",
       "3  1232651956528799747  2020-02-26 13:01:42   \n",
       "4  1231469470020972545   2020-02-23 6:42:55   \n",
       "\n",
       "                                           tweetText tweetLang  \\\n",
       "0  I love to look at people's opinions about the ...        es   \n",
       "1  The latest The AmericanAwaken ing Daily PM!  T...        en   \n",
       "2  #Macau casinos will be closed for half a month...        en   \n",
       "3  Social media conspiracies blame coronavirus on...        ca   \n",
       "4  Coronavirus, official: patient zero has nothin...        it   \n",
       "\n",
       "  tweetCoordinates tweetPlace  tweetSentiment  \n",
       "0                                           1  \n",
       "1                                          -1  \n",
       "2                                          -1  \n",
       "3                                          -1  \n",
       "4                                           0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feb_list = tsv_to_list(annotation_list[0])\n",
    "feb_labels = get_labels_for(feb_list)\n",
    "mar_list = tsv_to_list(annotation_list[1])\n",
    "mar_labels = get_labels_for(mar_list)\n",
    "apr_list = tsv_to_list(annotation_list[2])\n",
    "apr_labels = get_labels_for(apr_list)\n",
    "\n",
    "all_labels = feb_labels + mar_labels + apr_labels\n",
    "#print(all_labels.count(-1), all_labels.count(0), all_labels.count(1))\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(all_labels, columns=['tweetID', 'tweetTime', 'tweetText', 'tweetLang', 'tweetCoordinates', 'tweetPlace', 'tweetSentiment'])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 188, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 185, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 333, in run\n",
      "    reqs, check_supported_wheels=not options.target_dir\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\pip\\_internal\\resolution\\legacy\\resolver.py\", line 179, in resolve\n",
      "    discovered_reqs.extend(self._resolve_one(requirement_set, req))\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\pip\\_internal\\resolution\\legacy\\resolver.py\", line 362, in _resolve_one\n",
      "    abstract_dist = self._get_abstract_dist_for(req_to_install)\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\pip\\_internal\\resolution\\legacy\\resolver.py\", line 314, in _get_abstract_dist_for\n",
      "    abstract_dist = self.preparer.prepare_linked_requirement(req)\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 469, in prepare_linked_requirement\n",
      "    hashes=hashes,\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 259, in unpack_url\n",
      "    hashes=hashes,\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 130, in get_http_url\n",
      "    link, downloader, temp_dir.path, hashes\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 281, in _download_http_url\n",
      "    for chunk in download.chunks:\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 166, in iter\n",
      "    for x in it:\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 39, in response_chunks\n",
      "    decode_content=False,\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 564, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 507, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 65, in read\n",
      "    self._close()\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 52, in _close\n",
      "    self.__callback(self.__buf.getvalue())\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\controller.py\", line 309, in cache_response\n",
      "    cache_url, self.serializer.dumps(request, response, body=body)\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\serialize.py\", line 72, in dumps\n",
      "    return b\",\".join([b\"cc=4\", msgpack.dumps(data, use_bin_type=True)])\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\pip\\_vendor\\msgpack\\__init__.py\", line 35, in packb\n",
      "    return Packer(**kwargs).pack(o)\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\pip\\_vendor\\msgpack\\fallback.py\", line 936, in pack\n",
      "    self._pack(obj)\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\pip\\_vendor\\msgpack\\fallback.py\", line 920, in _pack\n",
      "    len(obj), dict_iteritems(obj), nest_limit - 1\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\pip\\_vendor\\msgpack\\fallback.py\", line 1021, in _pack_map_pairs\n",
      "    self._pack(v, nest_limit - 1)\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\pip\\_vendor\\msgpack\\fallback.py\", line 920, in _pack\n",
      "    len(obj), dict_iteritems(obj), nest_limit - 1\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\pip\\_vendor\\msgpack\\fallback.py\", line 1020, in _pack_map_pairs\n",
      "    self._pack(k, nest_limit - 1)\n",
      "  File \"c:\\program files\\python37\\lib\\site-packages\\pip\\_vendor\\msgpack\\fallback.py\", line 872, in _pack\n",
      "    return self._buffer.write(obj)\n",
      "MemoryError\n",
      "WARNING: You are using pip version 20.1.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\program files\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.4.1-cp37-cp37m-win_amd64.whl (370.7 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Package(s) not found: tensorflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras in c:\\users\\shery\\appdata\\roaming\\python\\python37\\site-packages (2.4.3)\n",
      "Requirement already satisfied: h5py in c:\\users\\shery\\appdata\\roaming\\python\\python37\\site-packages (from keras) (3.1.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\shery\\appdata\\roaming\\python\\python37\\site-packages (from keras) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\shery\\appdata\\roaming\\python\\python37\\site-packages (from keras) (1.19.0)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\shery\\appdata\\roaming\\python\\python37\\site-packages (from keras) (1.5.1)\n",
      "Requirement already satisfied: cached-property; python_version < \"3.8\" in c:\\users\\shery\\appdata\\roaming\\python\\python37\\site-packages (from h5py->keras) (1.5.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\program files\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-2c880b115a05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pip3 show tensorflow'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pip3 install keras'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     raise ImportError(\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "!pip3 install tensorflow\n",
    "!pip3 show tensorflow\n",
    "!pip3 install keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['tweetText'].values\n",
    "X= X.astype(str)\n",
    "y = df['tweetSentiment'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer() \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "num_words=9000\n",
    "tokenizer.word_index = {e:i for e,i in tokenizer.word_index.items() if i <= num_words} # <= because tokenizer is 1 indexed\n",
    "tokenizer.word_index[tokenizer.oov_token] = num_words + 1\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "maxlen = 278\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath,encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word]\n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix\n",
    "    \n",
    "embedding_dim = 300\n",
    "embedding_matrix = create_embedding_matrix('glove.6B.300d.txt', tokenizer.word_index, embedding_dim)\n",
    "\n",
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "embedding_accuracy = nonzero_elements / vocab_size\n",
    "print('embedding accuracy: ' + str(embedding_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=maxlen, trainable=True))\n",
    "model.add(layers.Conv1D(256, 3, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(28, activation='sigmoid'))\n",
    "opt = optimizers.Adam(lr=0.0002)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "         ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "res = model.fit(X_train, y_train, epochs=15, verbose=True, callbacks=callbacks, validation_data=(X_test, y_test), batch_size=100)\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds=[0.1,0.2,0.25,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for val in thresholds:\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='micro')\n",
    "    recall = recall_score(y_test, pred, average='micro')\n",
    "    f1 = f1_score(y_test, pred, average='micro')\n",
    "   \n",
    "    print(\"Threshold: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(val, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['pos', 'neg', 'neu']\n",
    "f1_scores = []\n",
    "threshold = 0.25\n",
    "for i in range(0,28):\n",
    "    emotion_prediction = y_pred[:,i]\n",
    "    emotion_prediction[emotion_prediction>=threshold]=1\n",
    "    emotion_prediction[emotion_prediction<threshold]=0\n",
    "    emotion_test = y_test[:,i]\n",
    "    precision = precision_score(emotion_test, emotion_prediction)\n",
    "    recall = recall_score(emotion_test, emotion_prediction)\n",
    "    f1 = f1_score(emotion_test, emotion_prediction)\n",
    "    f1_scores.append(f1)\n",
    "    print(\"Emotion: {}, Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(column_names[i], precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "plt.bar(column_names,f1_scores)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
